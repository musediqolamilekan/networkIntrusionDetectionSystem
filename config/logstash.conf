input {
  kafka {
    bootstrap_servers => "broker:29092"
    topics => ["network-packets"]
    codec => "json"
  }
}

filter {
  # Packet data processing
  fingerprint {
    source => ["packet_id"]
    target => "[@metadata][fingerprint]"
    method => "SHA1"
  }

  if [@metadata][fingerprint] {
    if [@metadata][fingerprint] in [@metadata][fingerprint_cache] {
      drop {}
    } else {
      aggregate {
        task_id => "%{[@metadata][fingerprint]}"
        code => "map['count'] ||= 0; map['count'] += 1;"
        map_action => "create"
      }
      mutate {
        add_field => {
          "deduplication_count" => "%{[count]}"
        }
      }
    }
  }

  # Convert string fields to appropriate data types
  mutate {
    convert => {
      "bytes_transmitted" => "integer"
      "src_port" => "integer"
      "dst_port" => "integer"
      "icmp_type" => "integer"
      "icmp_code" => "integer"
    }
  }

  # Normalize field names and structure
  mutate {
    rename => {
      "src_ip" => "[network][source][ip]"
      "dst_ip" => "[network][destination][ip]"
      "src_port" => "[network][source][port]"
      "dst_port" => "[network][destination][port]"
      "icmp_type" => "[network][icmp][type]"
      "icmp_code" => "[network][icmp][code]"
      "bytes_transmitted" => "[network][bytes_transmitted]"
    }
  }

  # Clean up fields by removing any unwanted or unnecessary fields
  mutate {
    remove_field => ["@version", "host"]
  }
}

output {
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    index => "network-packets-%{+YYYY.MM.dd}"
  }
  stdout { codec => rubydebug }
}
